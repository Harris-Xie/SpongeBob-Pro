"""
SpongeBob æ¨¡å‹äº¤äº’å¼å¯¹è¯è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
"""
import argparse
import torch
from transformers import AutoTokenizer, TextStreamer
from model.config import SpongeBobConfig
from model.model_spongebob_pro import SpongeBobForCausalLM

def main():
    parser = argparse.ArgumentParser(description="SpongeBobæ¨¡å‹äº¤äº’å¯¹è¯")
    parser.add_argument('--model_path', default='/apdcephfs_qy4/share_302593112/huaibingxie/SpongeBob/out_sft/exp_thinking/global_step_782/sft_768.pth', type=str, help="æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆ.pthæ–‡ä»¶ï¼‰")
    parser.add_argument('--tokenizer_path', default='./tokenizer_15k', type=str, help="Tokenizerè·¯å¾„")
    parser.add_argument('--model_type', default='sft', type=str, choices=['pretrain', 'sft'], help="æ¨¡å‹ç±»å‹ï¼špretrainï¼ˆæ–‡æœ¬ç»­å†™ï¼‰æˆ– sftï¼ˆå¯¹è¯ï¼‰")
    parser.add_argument('--hidden_size', default=768, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument('--num_hidden_layers', default=12, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument('--max_new_tokens', default=2048, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument('--temperature', default=0.7, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼ˆ0-1ï¼‰")
    parser.add_argument('--top_p', default=0.7, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str)
    args = parser.parse_args()
    
    # è‡ªåŠ¨æ¨æ–­æ¨¡å‹ç±»å‹ï¼ˆä»æ–‡ä»¶åï¼‰
    if 'pretrain' in args.model_path:
        args.model_type = 'pretrain'
    elif 'sft' in args.model_path:
        args.model_type = 'sft'
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    print(f'åŠ è½½æ¨¡å‹: {args.model_path}')
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    
    model = SpongeBobForCausalLM(SpongeBobConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers
    ))
    model.load_state_dict(torch.load(args.model_path, map_location=args.device))
    model.eval().to(args.device)
    
    print(f'âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼è®¾å¤‡: {args.device}')
    print(f'ğŸ“ æ¨¡å‹ç±»å‹: {args.model_type} ({"å¯¹è¯æ¨¡å¼" if args.model_type == "sft" else "æ–‡æœ¬ç»­å†™"})\n')
    print('='*60)
    print('ğŸ’¬ å¼€å§‹å¯¹è¯ (è¾“å…¥ exit é€€å‡º)')
    print('='*60)
    
    # å¯¹è¯å¾ªç¯
    conversation = []
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)
    
    while True:
        user_input = input('\nğŸ‘¤ ä½ : ').strip()
        
        if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
            print('ğŸ‘‹ å†è§ï¼')
            break
        
        if not user_input:
            continue
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ ¼å¼åŒ–è¾“å…¥
        if args.model_type == 'pretrain':
            # é¢„è®­ç»ƒæ¨¡å‹ï¼šç›´æ¥æ–‡æœ¬ç»­å†™ï¼Œä¸ä¿ç•™å†å²
            formatted_input = user_input
            conversation = []  # æ¸…ç©ºå†å²
        else:
            # SFTæ¨¡å‹ï¼šä½¿ç”¨ chat templateï¼Œä¿ç•™å†å²å¯¹è¯
            conversation.append({"role": "user", "content": user_input})
            formatted_input = tokenizer.apply_chat_template(
                conversation=conversation, 
                tokenize=False, 
                add_generation_prompt=True
            )
        
        inputs = tokenizer(formatted_input, return_tensors="pt").to(args.device)
        
        # ç”Ÿæˆå›å¤
        print('ğŸ§½ SpongeBob: ', end='', flush=True)
        with torch.no_grad():
            generated_ids = model.generate(
                inputs=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=args.max_new_tokens,
                do_sample=True,
                streamer=streamer,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                top_p=args.top_p,
                temperature=args.temperature,
                repetition_penalty=1.2
            )
        
        # è§£ç å›å¤å¹¶æ·»åŠ åˆ°å†å²
        response = tokenizer.decode(
            generated_ids[0][len(inputs["input_ids"][0]):], 
            skip_special_tokens=False
        )
        
        # åªæœ‰ SFT æ¨¡å‹æ‰ä¿ç•™å¯¹è¯å†å²
        if args.model_type == 'sft':
            conversation.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
